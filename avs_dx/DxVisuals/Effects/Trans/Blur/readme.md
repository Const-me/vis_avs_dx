I’ve found a method to implement Gaussian blur of arbitrary size spending slightly more than 1 texture read per pixel per pass (two passes are required). See LinearConvolutionCS.hlsl for the source, and LinearConvolution.txt for some ASCII “art” that explains how it works.The trick is optimizing the algorithm for RAM access patterns. BTW that’s typical in modern world with very fast processors, be it CPU or GPU, and, in comparison, very slow memory.On a desktop PC with GeForce 1080 Ti it takes 0.9ms to apply 19x19 Gaussian blur to 1920x1080 frame. On a netbook with Iris 550 the result is more modest, 8-9ms for FullHD frame. That result is roughly proportional to the raw performance, which is 10.6 TFlops for Nvidia versus just 0.84 TFlops for Intel.